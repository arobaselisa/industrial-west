{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b305bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import os\n",
    "import subprocess\n",
    "import  tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937562de",
   "metadata": {},
   "source": [
    "### The New York Herald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tar.bz2 files in global tar file\n",
    "tar = tarfile.open(\"../data/Institution.tar\")\n",
    "bz2_paths = [name for name in tar.getnames() if \"Institution (2)/United States/\" in name]\n",
    "\n",
    "# Create dataframe\n",
    "coal_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "\n",
    "total_articles_count = 0\n",
    "\n",
    "print(\"Creating the dataset (might take a few minutes) ...\")\n",
    "\n",
    "# open different US tar files one by one\n",
    "for p in bz2_paths[1:]: # exclude first odt file\n",
    "    subprocess.run([\"tar\", \"-xvf\", \"../data/Institution.tar\", p], stdout=subprocess.PIPE, text=True)\n",
    "    tar = tarfile.open(p)\n",
    "    for m in tar:\n",
    "        if \".txt\" in m.name:\n",
    "            total_articles_count += 1\n",
    "            f=tar.extractfile(m)\n",
    "            content=f.read()\n",
    "            if b'telegraph' in content:\n",
    "                pub_date = m.name.split(\"/\")[1] + \"-\" + m.name.split(\"/\")[2] + \"-\" + m.name.split(\"/\")[3] \n",
    "                coal_articles.loc[m.name.replace(\"/ocr.txt\", \"\")] = {\"publication_date\" : pub_date, \"content\":content.decode(\"utf-8\")}\n",
    "    # Removes files to save memory\n",
    "    subprocess.run([\"rm\", \"-rf\", \"Institution (2)\"])\n",
    "    coal_articles.to_csv(\"../data/telegraph_articles_us.csv\")\n",
    "\n",
    "print(\"Proportion of articles talking about steel \", len(coal_articles.index)/total_articles_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491deca2",
   "metadata": {},
   "source": [
    "### Le Figaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_article = glob.glob('../data/le_figaro/*/*/*.json')\n",
    "coal_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "for article in tqdm(every_article):\n",
    "    with open(article, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        if \" acier\" in unidecode.unidecode(str.lower(data[\"contentAsText\"][0])):\n",
    "            coal_articles.loc[\"figaro_\" + article.split(\"/\")[3]] = {\"publication_date\" : article.split(\"/\")[3], \"content\":data[\"contentAsText\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coal_articles)/len(every_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54156717",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal_articles[\"publication_date\"] = coal_articles.apply(lambda row: row[\"publication_date\"][:4]+\"-\"+row[\"publication_date\"][4:6]\n",
    "                                                        +\"-\"+row[\"publication_date\"][6:]\n",
    "                                                        , axis=1)\n",
    "coal_articles.to_csv(\"steel_articles_fr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe5da4",
   "metadata": {},
   "source": [
    "### El Imparcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_article = glob.glob('data/el_imparcial/*.txt')\n",
    "coal_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "for article in tqdm(every_article):\n",
    "    with open(article, 'r') as f:\n",
    "        data = \"\\n\".join(f.readlines())\n",
    "        if \"telegrafo\" in unidecode.unidecode(str.lower(data)):\n",
    "            coal_articles.loc[\"imparcial_\" + article.split(\"/\")[2].split(\"_\")[0]] = {\"publication_date\" : article.split(\"/\")[2].split(\"_\")[0][:4] + \"-\" + article.split(\"/\")[2].split(\"_\")[0][4:6] + \"-\" + article.split(\"/\")[2].split(\"_\")[0][6:8], \"content\":data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coal_articles)/len(every_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ecaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal_articles.to_csv(\"telegraph_articles_es.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9934c",
   "metadata": {},
   "source": [
    "### Neue Hamburger Zeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whole_text(file_name):\n",
    "    tree = ET.parse(file_name) \n",
    "    root = tree.getroot()\n",
    "    txt = \"\"\n",
    "    for textblock in root.findall(\".//{http://www.loc.gov/standards/alto/ns-v2#}TextBlock\"):\n",
    "        txt += \" \".join([s.attrib[\"CONTENT\"] for s in textblock.findall('.//{http://www.loc.gov/standards/alto/ns-v2#}String')])\n",
    "        txt += \"\\n\\n\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do this with the 3 hamburgers\n",
    "all_files = sorted(glob.glob(\"../Institution (2)/Hamburg/PPN689063377/*/*.xml\"))\n",
    "# Create dataframes\n",
    "coal_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "elec_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "telegraph_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "steel_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(all_files):\n",
    "    pub_date = file.split(\"_\")[2][:4] + \"-\" + file.split(\"_\")[2][4:6] + \"-\" + file.split(\"_\")[2][6:8]\n",
    "    text = get_whole_text(file)\n",
    "    for block in text.split(\"\\n\"):\n",
    "        if \"Kohle\" in block:\n",
    "            try :\n",
    "                coal_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": coal_articles.loc[\"hamburg_\" + pub_date].content + block}\n",
    "            except:\n",
    "                coal_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": block}\n",
    "        if \"Elektrizit√§t\" in block:\n",
    "            try :\n",
    "                elec_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": elec_articles.loc[\"hamburg_\" + pub_date].content + block}\n",
    "            except:\n",
    "                elec_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": block}\n",
    "        if \"Stahl\" in block:\n",
    "            try :\n",
    "                steel_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": steel_articles.loc[\"hamburg_\" + pub_date].content + block}\n",
    "            except:\n",
    "                steel_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": block}\n",
    "        if \"Telegraph\" in block:\n",
    "            try :\n",
    "                telegraph_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": telegraph_articles.loc[\"hamburg_\" + pub_date].content + block}\n",
    "            except:\n",
    "                telegraph_articles.loc[\"hamburg_\" + pub_date] = {\"publication_date\" : pub_date, \"content\": block}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fa8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coal_articles.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb025ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#every_german = pd.concat([data0, data1, data2], ignore_index=True, axis=0)\n",
    "every_german[\"id\"] = every_german.apply(lambda row: row[0].split(\"/\")[0], axis=1)\n",
    "every_german = every_german.groupby('id')[\"content\"].agg(lambda x: \"\\n\".join(x.tolist()))\n",
    "every_german = pd.DataFrame(every_german)\n",
    "every_german[\"publication_date\"] = every_german.index.to_list()\n",
    "every_german[\"publication_date\"] = every_german.apply(lambda row : row[\"publication_date\"].split(\"_\")[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_german.to_csv(\"coal_articles_de.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6a7aa",
   "metadata": {},
   "source": [
    "### La Stampa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"../data/Italian newspaper/out_fernandez/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kw_lines(lines, kw):\n",
    "    sentences = lines.split(\".\")\n",
    "    coal_idx = [idx for idx in range(len(sentences)) if kw in sentences[idx]]\n",
    "    coal_idx_aug = set(sum(list(map(lambda x: [max(0, x-1), x, min(x+1, len(sentences)-1)], coal_idx)), []))\n",
    "    return \" \".join([sentences[i] for i in coal_idx_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal_articles = pd.DataFrame(columns=[\"publication_date\", \"content\"])\n",
    "kw = \"elettricit√†\"\n",
    "for file in tqdm(all_files):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        if len(lines) > 0:\n",
    "            year = file.split(\"_\")[-1]\n",
    "            for i, l in enumerate(lines):\n",
    "                content = get_kw_lines(l, kw)\n",
    "                if content != \"\":\n",
    "                    coal_articles.loc[\"stampa_\" + str(year) + \"_\" + str(i)] = {\"publication_date\" : year, \"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d28496",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal_articles.to_csv(\"../data/elec_articles_it.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
