{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to create networks using the WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tExt0fHf-67X",
    "outputId": "61155de9-04cb-4f99-bee8-f72c2d2c5a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlOl9FBl-83b",
    "outputId": "a918628f-dd4d-4d3e-dc32-83bfae6a414b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/MUSE\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mg_jbHiAAd6Z",
    "outputId": "bd86a73f-bfe6-4383-cf60-74803763d01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wmd\n",
      "  Downloading wmd-1.3.2.tar.gz (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from wmd) (1.21.6)\n",
      "Building wheels for collected packages: wmd\n",
      "  Building wheel for wmd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wmd: filename=wmd-1.3.2-cp38-cp38-linux_x86_64.whl size=629451 sha256=5bdf54517c1eca356c3472148f064f732f85f0976b199f51403d48b5fe44aecf\n",
      "  Stored in directory: /root/.cache/pip/wheels/eb/4c/cd/40ec1e13bfd149162c9a69f5b07728410ea9af264e66cea28d\n",
      "Successfully built wmd\n",
      "Installing collected packages: wmd\n",
      "Successfully installed wmd-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-CVdyz3p9yE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wmd import WMD\n",
    "from networkx.algorithms import community\n",
    "from google.colab import output\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtDVwEz5OGCb"
   },
   "outputs": [],
   "source": [
    "topics_df = pd.read_csv(\"../data/topics_df.csv\")\n",
    "to_list = lambda l : l.topic.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\'\", \"\").split(\", \")\n",
    "topics_df[\"topic_id\"] = topics_df.apply(lambda row: row[\"country\"] + \"_\" + row[\"period\"] + \"_\" + to_list(row)[0] + \"_\" + to_list(row)[1] + \"_\" + to_list(row)[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NCIIDQjAhqG"
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gAmMXMmgAngQ"
   },
   "outputs": [],
   "source": [
    "embeddings_fr, id2word_fr, word2id_fr = load_vec(\"./dumped/debug/g3n9myrpjg/vectors-fr.txt\")\n",
    "embeddings_us, id2word_us, word2id_us = load_vec(\"./dumped/debug/g3n9myrpjg/vectors-en.txt\")\n",
    "embeddings_de, id2word_de, word2id_de = load_vec(\"./dumped/debug/q0t6x4r8j2/vectors-de.txt\")\n",
    "embeddings_it, id2word_it, word2id_it = load_vec(\"./dumped/debug/ebomrrgqiy/vectors-it.txt\")\n",
    "embeddings_es, id2word_es, word2id_es = load_vec(\"./dumped/debug/wzpg359r5l/vectors-es.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zV9XjWg0Cffn"
   },
   "outputs": [],
   "source": [
    "def add_embedding(t, word2id, embd, reduced_emb, reduced_word2id, idx):\n",
    "    for w in to_list(t[1]):\n",
    "          if w in word2id.keys():\n",
    "              reduced_emb = [embd[word2id[w]]] if not len(reduced_emb) else reduced_emb + [embd[word2id[w]]]\n",
    "              reduced_word2id[w] = idx\n",
    "              idx += 1\n",
    "    return reduced_emb, reduced_word2id, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FRWenM2pJat1"
   },
   "outputs": [],
   "source": [
    "def create_one_nbow(topic, reduced_word2id):\n",
    "    bow0 = []\n",
    "    w0 = [] #np.zeros((len(reduced_emb)), dtype=np.float32)\n",
    "    for i, w in enumerate(topic):\n",
    "        try :\n",
    "            bow0 = [reduced_word2id[w]] if not len(bow0) else bow0 + [reduced_word2id[w]]\n",
    "            w0 += [1 - 0.1*i]\n",
    "        except :\n",
    "            pass\n",
    "    return bow0, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BqYlM8Q8J9fO"
   },
   "outputs": [],
   "source": [
    "def compute_graph_data(topics_kw, threshold, threshold_relax):\n",
    "    # aggregate all embeddings together\n",
    "    reduced_emb = []\n",
    "    reduced_word2id = dict()\n",
    "    idx = 0\n",
    "    for t in topics_kw[topics_kw[\"country\"]==\"fr\"].iterrows():\n",
    "        reduced_emb, reduced_word2id, idx = add_embedding(t, word2id_fr, embeddings_fr, reduced_emb, reduced_word2id, idx)\n",
    "    for t in topics_kw[topics_kw[\"country\"]==\"us\"].iterrows():\n",
    "        reduced_emb, reduced_word2id, idx = add_embedding(t, word2id_us, embeddings_us, reduced_emb, reduced_word2id, idx)\n",
    "    for t in topics_kw[topics_kw[\"country\"]==\"it\"].iterrows():\n",
    "        reduced_emb, reduced_word2id, idx = add_embedding(t, word2id_it, embeddings_it, reduced_emb, reduced_word2id, idx)\n",
    "    for t in topics_kw[topics_kw[\"country\"]==\"de\"].iterrows():\n",
    "        reduced_emb, reduced_word2id, idx = add_embedding(t, word2id_de, embeddings_de, reduced_emb, reduced_word2id, idx)\n",
    "    for t in topics_kw[topics_kw[\"country\"]==\"es\"].iterrows():\n",
    "        reduced_emb, reduced_word2id, idx = add_embedding(t, word2id_es, embeddings_es, reduced_emb, reduced_word2id, idx)\n",
    "    reduced_emb = np.array(reduced_emb, dtype=np.float32)\n",
    "\n",
    "    # create nbow\n",
    "    nbow = dict()\n",
    "    for i, t in enumerate(topics_kw[topics_kw[\"country\"]==\"fr\"].iterrows()):\n",
    "        bow, w = create_one_nbow(to_list(t[1]), reduced_word2id)\n",
    "        nbow[t[1].topic_id] = (t[1].topic_id, bow, np.ones(len(w))/len(w))\n",
    "    for i, t in enumerate(topics_kw[topics_kw[\"country\"]==\"us\"].iterrows()):\n",
    "        bow, w = create_one_nbow(to_list(t[1]), reduced_word2id)\n",
    "        nbow[t[1].topic_id] = (t[1].topic_id, bow, np.ones(len(w))/len(w))\n",
    "    for i, t in enumerate(topics_kw[topics_kw[\"country\"]==\"it\"].iterrows()):\n",
    "        bow, w = create_one_nbow(to_list(t[1]), reduced_word2id)\n",
    "        nbow[t[1].topic_id] = (t[1].topic_id, bow, np.ones(len(w))/len(w))\n",
    "    for i, t in enumerate(topics_kw[topics_kw[\"country\"]==\"de\"].iterrows()):\n",
    "        bow, w = create_one_nbow(to_list(t[1]), reduced_word2id)\n",
    "        nbow[t[1].topic_id] = (t[1].topic_id, bow, np.ones(len(w))/len(w))\n",
    "    for i, t in enumerate(topics_kw[topics_kw[\"country\"]==\"es\"].iterrows()):\n",
    "        bow, w = create_one_nbow(to_list(t[1]), reduced_word2id)\n",
    "        nbow[t[1].topic_id] = (t[1].topic_id, bow, np.ones(len(w))/len(w))\n",
    "    \n",
    "    # https://github.com/src-d/wmd-relax\n",
    "    reduced_emb_t = np.array(reduced_emb, dtype=np.float32)\n",
    "    calc = WMD(reduced_emb_t, nbow, vocabulary_min=3, vocabulary_max=2000)\n",
    "\n",
    "    # calculate weight between each topic \n",
    "    graph_topics_df = pd.DataFrame(columns = [\"from\", \"to\", \"weigth\"])\n",
    "    for t in topics_kw.iterrows():\n",
    "        topic = t[1].topic_id\n",
    "        nn = calc.nearest_neighbors(topic, k=100, early_stop=0.99)\n",
    "        output.clear()\n",
    "        for n in nn:\n",
    "            # same country\n",
    "            if (t[1].topic_id.split(\"_\")[0] == topics_kw[topics_kw[\"topic_id\"]==n[0]].iloc[0].topic_id.split(\"_\")[0]) and n[1] < threshold:\n",
    "                graph_topics_df.loc[len(graph_topics_df.index)] = [topic,topics_kw[topics_kw[\"topic_id\"]==n[0]].iloc[0].topic_id, threshold - n[1]]\n",
    "            if not(t[1].topic_id.split(\"_\")[0] == topics_kw[topics_kw[\"topic_id\"]==n[0]].iloc[0].topic_id.split(\"_\")[0]) and n[1] < threshold_relax:\n",
    "                graph_topics_df.loc[len(graph_topics_df.index)] = [topic,topics_kw[topics_kw[\"topic_id\"]==n[0]].iloc[0].topic_id, threshold_relax - n[1]]\n",
    "\n",
    "    return graph_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-l_N8R6MmSOh"
   },
   "outputs": [],
   "source": [
    "def find_communities(G):\n",
    "    communities_greedy = community.greedy_modularity_communities(G)\n",
    "    cov_greedy, perf_greedy = community.partition_quality(G, communities_greedy)\n",
    "    mod_greedy = community.modularity(G, communities_greedy)\n",
    "    communities_louvain = community.louvain_communities(G)\n",
    "    cov_louvain, perf_louvain = community.partition_quality(G, communities_louvain)\n",
    "    mod_louvain = community.modularity(G, communities_louvain)\n",
    "    return cov_greedy, perf_greedy, mod_greedy, cov_louvain, perf_louvain, mod_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaH3dgD1hfif"
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.5,2.5, 0.25)\n",
    "communities_df = pd.DataFrame(columns=[\"keyword\", \"threshold\", \"threshold_relax\", \"algo\", \"modularity\", \"performance\", \"coverage\", \"nodes\"])\n",
    "\n",
    "for t in thresholds:\n",
    "  for tr in np.arange(t, 2.5, 0.25):\n",
    "    graph_telegraph_df = compute_graph_data(topics_df[topics_df[\"keyword\"]==\"telegraph\"], threshold=t, threshold_relax=tr)\n",
    "    graph_steel_df = compute_graph_data(topics_df[topics_df[\"keyword\"]==\"steel\"], threshold=t, threshold_relax=tr)\n",
    "    graph_elec_df = compute_graph_data(topics_df[topics_df[\"keyword\"]==\"elec\"], threshold=t, threshold_relax=tr)\n",
    "    graph_coal_df = compute_graph_data(topics_df[topics_df[\"keyword\"]==\"coal\"], threshold=t, threshold_relax=tr)\n",
    "    \n",
    "    G_telegraph = nx.from_pandas_edgelist(graph_telegraph_df, source='from', target='to')\n",
    "    G_steel = nx.from_pandas_edgelist(graph_steel_df, source='from', target='to')\n",
    "    G_elec = nx.from_pandas_edgelist(graph_elec_df, source='from', target='to')\n",
    "    G_coal = nx.from_pandas_edgelist(graph_coal_df, source='from', target='to')\n",
    "\n",
    "    try:\n",
    "        cov_greedy, perf_greedy, mod_greedy, cov_louvain, perf_louvain, mod_louvain = find_communities(G_telegraph)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"telegraph\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": mod_greedy, \"performance\": perf_greedy, \"coverage\": cov_greedy, \"nodes\": G_telegraph.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"telegraph\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": mod_louvain, \"performance\": perf_louvain, \"coverage\": cov_louvain, \"nodes\": G_telegraph.number_of_nodes()},ignore_index=True)\n",
    "    except:\n",
    "        communities_df = communities_df.append({\"keyword\" : \"telegraph\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_telegraph.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"telegraph\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_telegraph.number_of_nodes()},ignore_index=True)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        cov_greedy, perf_greedy, mod_greedy, cov_louvain, perf_louvain, mod_louvain = find_communities(G_steel)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"steel\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": mod_greedy, \"performance\": perf_greedy, \"coverage\": cov_greedy, \"nodes\": G_steel.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"steel\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": mod_louvain, \"performance\": perf_louvain, \"coverage\": cov_louvain, \"nodes\":  G_steel.number_of_nodes()},ignore_index=True)\n",
    "    except:\n",
    "        communities_df = communities_df.append({\"keyword\" : \"steel\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_steel.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"steel\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_steel.number_of_nodes()},ignore_index=True)\n",
    "\n",
    "    try:\n",
    "        cov_greedy, perf_greedy, mod_greedy, cov_louvain, perf_louvain, mod_louvain = find_communities(G_elec)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"elec\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": mod_greedy, \"performance\": perf_greedy, \"coverage\": cov_greedy, \"nodes\": G_elec.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"elec\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": mod_louvain, \"performance\": perf_louvain, \"coverage\": cov_louvain, \"nodes\": G_elec.number_of_nodes()},ignore_index=True)\n",
    "    except:\n",
    "        communities_df = communities_df.append({\"keyword\" : \"elec\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_elec.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"elec\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_elec.number_of_nodes()},ignore_index=True)\n",
    "\n",
    "\n",
    "    try:\n",
    "        cov_greedy, perf_greedy, mod_greedy, cov_louvain, perf_louvain, mod_louvain = find_communities(G_coal)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"coal\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": mod_greedy, \"performance\": perf_greedy, \"coverage\": cov_greedy, \"nodes\": G_coal.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"coal\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": mod_louvain, \"performance\": perf_louvain, \"coverage\": cov_louvain, \"nodes\": G_coal.number_of_nodes()},ignore_index=True)\n",
    "    except:\n",
    "        communities_df = communities_df.append({\"keyword\" : \"coal\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"greedy\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_coal.number_of_nodes()},ignore_index=True)\n",
    "        communities_df = communities_df.append({\"keyword\" : \"coal\", \"threshold\" : t, \"threshold_relax\" : tr, \"algo\" : \"louvain\", \"modularity\": \"Nan\", \"performance\": \"Nan\", \"coverage\": \"Nan\", \"nodes\": G_coal.number_of_nodes()},ignore_index=True)\n",
    "\n",
    "    communities_df.to_csv(\"../data/communities_df.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6hWRmmpoSHn"
   },
   "outputs": [],
   "source": [
    "communities_df = pd.read_csv(\"../communities_df.csv\")\n",
    "communities_df[\"nodes_percentage\"] = communities_df.apply(lambda row: row[\"nodes\"]/len(topics_df[topics_df[\"keyword\"]==row[\"keyword\"]].index), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5dEx_mCr3fF"
   },
   "outputs": [],
   "source": [
    "reduced_comm_df = communities_df[communities_df[\"nodes_percentage\"] > 0.45]\n",
    "reduced_comm_df[[\"modularity\", \"performance\", \"coverage\"]] = reduced_comm_df[[\"modularity\", \"performance\", \"coverage\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPJuFnXvf8LE"
   },
   "outputs": [],
   "source": [
    "def find_best_thresholds(df, keyword):\n",
    "    df = df[df[\"keyword\"]==keyword]\n",
    "    df[\"score\"] = df.apply(lambda row: 0.7*row[\"modularity\"] + 0.15*row[\"performance\"] +0.15*row[\"coverage\"], axis=1)\n",
    "    t = df.loc[df[\"score\"].sort_values().tail(1).index[0]][\"threshold\"]\n",
    "    tr = df.loc[df[\"score\"].sort_values().tail(1).index[0]][\"threshold_relax\"]\n",
    "    return t, tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvuW_AKQg8kF"
   },
   "outputs": [],
   "source": [
    "t_steel, tr_steel = find_best_thresholds(reduced_comm_df, \"steel\") #louvain\n",
    "t_coal, tr_coal = find_best_thresholds(reduced_comm_df, \"coal\") #greedy\n",
    "t_telegraph, tr_telegraph = find_best_thresholds(reduced_comm_df, \"telegraph\") #greedy\n",
    "t_elec, tr_elec = find_best_thresholds(reduced_comm_df, \"elec\") #louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCN7q5hiPcOw"
   },
   "outputs": [],
   "source": [
    "print(t_steel, tr_steel)\n",
    "print(t_coal, tr_coal)\n",
    "print(t_telegraph, tr_telegraph)\n",
    "print(t_elec, tr_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oc9BkGm3hcPR"
   },
   "outputs": [],
   "source": [
    "# Save Graphs\n",
    "compute_graph_data(topics_df[topics_df[\"keyword\"]==\"steel\"], t_steel, tr_steel).to_csv(\"graph_steel_df.csv\", index=False)\n",
    "compute_graph_data(topics_df[topics_df[\"keyword\"]==\"coal\"], t_coal, tr_coal).to_csv(\"graph_coal_df.csv\", index=False)\n",
    "compute_graph_data(topics_df[topics_df[\"keyword\"]==\"telegraph\"], t_telegraph, tr_telegraph).to_csv(\"graph_telegraph_df.csv\", index=False)\n",
    "compute_graph_data(topics_df[topics_df[\"keyword\"]==\"elec\"], t_elec, tr_elec).to_csv(\"graph_elec_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
