{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3d3f7c",
   "metadata": {},
   "source": [
    "A notebook to evaluate different kinds of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b140641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import helpers\n",
    "import io\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce9ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Create corpus for model...\n",
      "Runtime: 0.06 seconds || Completed: 1 of 12420\n",
      "Runtime: 82.01 seconds || Completed: 501 of 12420\n",
      "Runtime: 151.57 seconds || Completed: 1001 of 12420\n",
      "Runtime: 215.81 seconds || Completed: 1501 of 12420\n",
      "Runtime: 264.82 seconds || Completed: 2001 of 12420\n",
      "Runtime: 384.04 seconds || Completed: 2501 of 12420\n",
      "Runtime: 482.14 seconds || Completed: 3001 of 12420\n",
      "Runtime: 558.92 seconds || Completed: 3501 of 12420\n",
      "Runtime: 647.27 seconds || Completed: 4001 of 12420\n",
      "Runtime: 715.03 seconds || Completed: 4501 of 12420\n",
      "Runtime: 788.01 seconds || Completed: 5001 of 12420\n",
      "Runtime: 849.41 seconds || Completed: 5501 of 12420\n",
      "Runtime: 918.71 seconds || Completed: 6001 of 12420\n",
      "Runtime: 1011.83 seconds || Completed: 6501 of 12420\n",
      "Runtime: 1098.10 seconds || Completed: 7001 of 12420\n",
      "Runtime: 1170.24 seconds || Completed: 7501 of 12420\n",
      "Runtime: 1252.58 seconds || Completed: 8001 of 12420\n",
      "Runtime: 1312.53 seconds || Completed: 8501 of 12420\n",
      "Runtime: 1371.87 seconds || Completed: 9001 of 12420\n",
      "Runtime: 1402.54 seconds || Completed: 9501 of 12420\n",
      "Runtime: 1433.49 seconds || Completed: 10001 of 12420\n",
      "Runtime: 1460.45 seconds || Completed: 10501 of 12420\n",
      "Runtime: 1491.40 seconds || Completed: 11001 of 12420\n",
      "Runtime: 1516.63 seconds || Completed: 11501 of 12420\n",
      "Runtime: 1553.48 seconds || Completed: 12001 of 12420\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print(\"Load data...\")\n",
    "DATA_PATH = \"./data/telegraph_articles_de.csv\"\n",
    "SPACY_LANGUAGE = \"de_core_news_sm\"\n",
    "data_de = pd.read_csv(DATA_PATH, index_col=\"Unnamed: 0\") \n",
    "print(\"Create corpus for model...\")\n",
    "raw_docs, _ = helpers.create_corpus(data_de, SPACY_LANGUAGE)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534923d",
   "metadata": {},
   "source": [
    "### 1. Different word embeddings to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd765bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 2 Vec skipgram\n",
    "w2v_sg_model = Word2Vec(sentences=raw_docs,\n",
    "                       vector_size=100\n",
    "                       , window=5\n",
    "                       , min_count=1\n",
    "                       , workers=4\n",
    "                       , sg=1 #skipgram\n",
    "                       , negative=5\n",
    "                       , seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d97f4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 2 Vec cbow\n",
    "w2v_cbow_model = Word2Vec(sentences=raw_docs,\n",
    "                       vector_size=100\n",
    "                       , window=5\n",
    "                       , min_count=1\n",
    "                       , workers=4\n",
    "                       , sg=0 #cbow\n",
    "                       , negative=5\n",
    "                       , seed = 0) #use of negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cef338cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext\n",
    "ftt_model = FastText(vector_size=100, \n",
    "                     window=5, \n",
    "                     min_count=1, \n",
    "                     sentences=raw_docs, \n",
    "                     epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818f79e",
   "metadata": {},
   "source": [
    "### 2. The topics to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c197a84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_df = pd.read_csv(\"../data/topics_df.csv\")\n",
    "test_topics = topics_df[(topics_df[\"country\"]==\"de\") & (topics_df[\"keyword\"]==\"telegraph\")][\"topic\"]\n",
    "to_list = lambda l : l.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\'\", \"\").split(\", \")\n",
    "test_topics = [to_list(row) for row in list(test_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c566e",
   "metadata": {},
   "source": [
    "### 3. The score measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4978aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compactness score of one word\n",
    "# https://aclanthology.org/W16-2508.pdf\n",
    "# Calculating by averaging all the pairwise semantic similarities of the words in the topic without this word\n",
    "def compactness(w, topic, model):\n",
    "    pairwise_sim = [1 - spatial.distance.cosine(model.wv[w],model.wv[w_bis]) for w_bis in topic if not w == w_bis]\n",
    "    return sum(pairwise_sim)/(len(pairwise_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7ed7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average compactness score for model\n",
    "def average_compactness(model, topics_list):\n",
    "    compactnesses = [compactness(w, t, model) for t in topics_list for w in t ]\n",
    "    return sum(compactnesses)/len(compactnesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7e79307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Position Percentage, same paper\n",
    "def mOPP(model, topics_list):\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for i, t in enumerate(topics_list):\n",
    "        c = [compactness(w, t, model) for w in t]\n",
    "        mean_for_group = sum(c)/len(c)\n",
    "        for j, t_bis in enumerate(topics_list):\n",
    "            if not i == j:\n",
    "                for w in t_bis:\n",
    "                    denom += 1\n",
    "                    if compactness(w, t, model) > mean_for_group: # good\n",
    "                        num += 1\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed1cff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(model, topics_list):\n",
    "    # compactness of word in its topic / in the other topics, for every word, mean\n",
    "    # the smaller the better (in its topic must be low, in other topics must be high)\n",
    "    ratios = []\n",
    "    for i, t in enumerate(topics_list):\n",
    "        for w in t:\n",
    "            cw = compactness(w, t, model)\n",
    "            for j, t_bis in enumerate(topics_list):\n",
    "                if not i == j:\n",
    "                    cw_bis = compactness(w, t_bis, model)\n",
    "                    ratios = [cw/cw_bis] if not len(ratios) else ratios + [cw_bis]\n",
    "    return sum(ratios)/len(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590f47e",
   "metadata": {},
   "source": [
    "### 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b80fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Compactness\n",
      "  Word2Vec CBOW model :  0.7200669950644879\n",
      "  Word2Vec Skipgram model :  0.5144439304068062\n",
      "  Fasttext model :  0.2043688678386321\n",
      "OPP\n",
      "  Word2Vec CBOW model :  0.13917867764813543\n",
      "  Word2Vec Skipgram model :  0.13070235644017386\n",
      "  Fasttext model :  0.10911690688629604\n",
      "Custom\n",
      "  Word2Vec CBOW model :  0.5461314424042428\n",
      "  Word2Vec Skipgram model :  0.3528988816537987\n",
      "  Fasttext model :  0.06661156495706871\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Compactness\")\n",
    "print(\"  Word2Vec CBOW model : \", average_compactness(w2v_cbow_model, test_topics))\n",
    "print(\"  Word2Vec Skipgram model : \", average_compactness(w2v_sg_model, test_topics))\n",
    "print(\"  Fasttext model : \", average_compactness(ftt_model, test_topics))\n",
    "print(\"OPP\")\n",
    "print(\"  Word2Vec CBOW model : \", mOPP(w2v_cbow_model, test_topics))\n",
    "print(\"  Word2Vec Skipgram model : \", mOPP(w2v_sg_model, test_topics))\n",
    "print(\"  Fasttext model : \", mOPP(ftt_model, test_topics))\n",
    "print(\"Custom\")\n",
    "print(\"  Word2Vec CBOW model : \", custom_score(w2v_cbow_model, test_topics))\n",
    "print(\"  Word2Vec Skipgram model : \", custom_score(w2v_sg_model, test_topics))\n",
    "print(\"  Fasttext model : \", custom_score(ftt_model, test_topics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
